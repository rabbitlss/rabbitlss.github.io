
# 1 什么是逻辑回归？

逻辑回归一般指logistic回归，是一种广义的线性回归（generalized linear model），常用于数据挖掘，疾病自动诊断，经济预测等领域。虽然名字里带“回归”，但是它实际上是一种分类方法，不仅能进行分类，还能获取每个类别的概率预测值。常用于两分类问题。
logistic回归通过函数L将w‘x+b对应一个隐状态p，p =L(w‘x+b),然后根据p 与1-p的大小决定因变量的值。如果L是logistic函数，就是logistic回归，如果L是多项式函数就是多项式回归。

# 2 逻辑回归的损失函数

## 2.1逻辑回归为何使用Sigmoid函数

Sigmoid函数形式如下：
![Sigmoid函数图像](https://img-blog.csdnimg.cn/20200819202003383.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI2ODU3OTA3,size_16,color_FFFFFF,t_70#pic_center)
对于函数g(z)=1(1+e−z)g(z)=\frac{1}{(1+e^{-z})}*g*(*z*)=(1+*e*−*z*)1​，当z≥0 时,y≥0.5,分类为1，当 z<0时,y<0.5,分类为0。
使用Sigmoid函数原因有两个方面：
1）Sigmoid 函数自身的性质
a. sigmoid 函数连续，单调递增
b. sigmiod 函数关于（0，0.5） 中心对称
c. 对sigmoid函数求导后计算速度快
p=ex1+exp=ex1+exp=ex1+exp=ex1+ex*p*=*e**x*1+*e**x**p*=*e**x*1+*e**x*
p′=p∗(1−p)p′=p∗(1−p)p′=p∗(1−p)p′=p∗(1−p)*p*′=*p*∗(1−*p*)*p*′=*p*∗(1−*p*)
2）指数族
logistic回归的损失函数为非指数族，逻辑回归认为函数其概率服从伯努利分布，将其写成指数族分布的形式。

## 2.2逻辑回归的步骤

（1）构造预测函数h
（2）构造损失函数J
（3）令J最小并求得回归参数w

## 2.3 预测函数的确定

逻辑回归主要用于二分类问题（即输出只有两种，分别代表两个类别），所以利用了Logistic函数（或称为Sigmoid函数），函数形式为：
g(z)=1(1+e−z)g(z)=\frac{1}{(1+e^{-z})}*g*(*z*)=(1+*e*−*z*)1​

逻辑回归从其原理上来说，其实是实现了一个决策边界：

$θ0x0+θ1x1+,...,+θnxn=∑ni=0θixi=θTx\theta_0x_0+\theta_1x_1+,...,+\theta_nx_n= \sum_{i=0}^n\theta_ix_i=\theta^Tx*θ*0*x*0+*θ*1*x*1+,...,+*θ**n**x**n*=*i*=0∑*n**θ**i**x**i*=*θ**T**x$*
构造预测函数：（将回归方程写入其中）
$hθ(x)=g(θTx)=1(1+e−θTx)h_\theta(x)=g(\theta^Tx )=\frac{1}{(1+e^{-\theta^Tx })}*h**θ*(*x*)=*g*(*θ**T**x*)=(1+*e*−*θ**T**x*)1
p=p(y=1∣x,θ)=hθ(x,θ)=11+e−(w0+θTx)p=p(y=1|x,\theta)=h_\theta(x,\theta)=\frac{1}{1+e^{-(w_0+\theta^Tx)}}*p*=*p*(*y*=1∣*x*,*θ*)=*h**θ*(*x*,*θ*)=1+*e*−(*w*0+*θ**T**x*)1$

函数$hθ(x,θ)h_\theta(x,\theta)*h**θ*(*x*,*θ*)$的值，表示结果取1的概率，所以对于因变量x,分类结果为1和0的概率分别为：
$p(y=1∣x,θ)=hθ(x,θ)p(y=1|x,\theta)=h_\theta(x,\theta)*p*(*y*=1∣*x*,*θ*)=*h**θ*(*x*,*θ*)p(y=0∣x,θ)=1−hθ(x,θ)p(y=0|x,\theta)=1-h_\theta(x,\theta)*p*(*y*=0∣*x*,*θ*)=1−*h**θ*(*x*,*θ*)$

## 2.4损失函数的构造

上边两式可改写为：
$p(y∣x,θ)=hθ(x,θ)y(1−hθ(x,θ))1−yp(y|x,\theta)=h_\theta(x,\theta)^y(1-h_\theta(x,\theta))^{1-y}*p*(*y*∣*x*,*θ*)=*h**θ*(*x*,*θ*)*y*(1−*h**θ*(*x*,*θ*))1−*y*上式是在参数θ\theta*θ*下，元组类标号为y的后验概率。假设现在已经得到了一个抽样样本，那么联合概率∏ni=1p(yi∣Xi;θ)\prod_{i=1}^np(y_i|X_i;\theta)∏*i*=1*n**p*(*y**i*∣*X**i*;*θ*)$的大小就可以反映模型的代价。联合概率的值越大，代价函数越小。

联合概率公式中采用连乘的方法不好计算，可用极大似然估计的方法进行求解，得到最大的参数θ\theta*θ*，这个最大的参数将是最佳参数，使得联合概率的值最大，代价值最小。因此就得到了逻辑回归的损失函数：
L(θ)=∏ni=1P(yi∣xi;θ)=∏ni=1(hθ(xi,θ))yi(1−hθ(xi,θ))1−yiL(\theta)=\prod_{i=1}^nP(y_i|x_i;\theta)=\prod_{i=1}^n(h_\theta(x_i,\theta))^{y_i}(1-h_\theta(x_i,\theta))^{1-y_i}*L*(*θ*)=*i*=1∏*n*​*P*(*y**i*​∣*x**i*​;*θ*)=*i*=1∏*n*​(*h**θ*​(*x**i*​,*θ*))*y**i*​(1−*h**θ*​(*x**i*​,*θ*))1−*y**i*​损失函数的对数似然函数为：
l(θ)=logL(θ)=∑ni=1(yiloghθ(xi,θ))+((1−yi)log(1−hθ(xi,θ)))l(\theta)=logL(\theta)=\sum_{i=1}^n(y_ilogh_\theta(x_i,\theta))+((1-y_i)log(1-h_\theta(x_i,\theta)))*l*(*θ*)=*l**o**g**L*(*θ*)=*i*=1∑*n*​(*y**i*​*l**o**g**h**θ*​(*x**i*​,*θ*))+((1−*y**i*​)*l**o**g*(1−*h**θ*​(*x**i*​,*θ*)))
得到的这个函数越大,证明我们得到的W就越好.因为在函数最优化的时候习惯让一个函数越小越好,所以我们在前边加一个负号.得到逻辑回归的损失函数公式如下:
J(θ)=−l(θ)=−∑ni=1(yiloghθ(xi,θ))−((1−yi)log(1−hθ(xi,θ)))J(\theta)=-l(\theta)=-\sum_{i=1}^n(y_ilogh_\theta(x_i,\theta))-((1-y_i)log(1-h_\theta(x_i,\theta)))*J*(*θ*)=−*l*(*θ*)=−*i*=1∑*n*​(*y**i*​*l**o**g**h**θ*​(*x**i*​,*θ*))−((1−*y**i*​)*l**o**g*(1−*h**θ*​(*x**i*​,*θ*)))

# 3 逻辑回归的模型

对于模型的训练而言：实质上来说就是利用数据求解出对应的模型的特定的ω。从而得到一个针对于当前数据的特征逻辑回归模型。

## 3.1模型求解与推导

模型回归得到的是一个最优化模型，求解方法有梯度下降法、牛顿法等，这里求解逻辑回归的损失函数，采用最基本的方法——梯度下降法。
求解步骤如下:

1-随机一组W.
2-将W带入交叉熵损失函数,让得到的点沿着负梯度的方向移动.
3-循环第二步.

## 3.2最终模型

待补充
