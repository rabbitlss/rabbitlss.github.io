<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>News_深度学习BERT - Even - A super concise theme for Hugo</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="rabbitlss" /><meta name="description" content="Transformer Transformer是一种完全基于Attention机制来加速深度学习训练过程的算法模型，其最大的优势在于其在并行化处理上做出的贡献。换" /><meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.73.0 with theme even" />


<link rel="canonical" href="https://rabbitlss.github.io/post/nlp%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E6%9E%90%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A02/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">



<link href="/sass/main.min.78f8f17bab244b9ee62ad16480c9584d5fc2db06ae20681d1ca225cefd80767c.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:title" content="News_深度学习BERT" />
<meta property="og:description" content="Transformer Transformer是一种完全基于Attention机制来加速深度学习训练过程的算法模型，其最大的优势在于其在并行化处理上做出的贡献。换" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://rabbitlss.github.io/post/nlp%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E6%9E%90%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A02/" />
<meta property="article:published_time" content="2020-08-04T23:10:23+08:00" />
<meta property="article:modified_time" content="2020-08-04T23:50:23+08:00" />
<meta itemprop="name" content="News_深度学习BERT">
<meta itemprop="description" content="Transformer Transformer是一种完全基于Attention机制来加速深度学习训练过程的算法模型，其最大的优势在于其在并行化处理上做出的贡献。换">
<meta itemprop="datePublished" content="2020-08-04T23:10:23&#43;08:00" />
<meta itemprop="dateModified" content="2020-08-04T23:50:23&#43;08:00" />
<meta itemprop="wordCount" content="6123">



<meta itemprop="keywords" content="preview,NLP,tag-5," /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="News_深度学习BERT"/>
<meta name="twitter:description" content="Transformer Transformer是一种完全基于Attention机制来加速深度学习训练过程的算法模型，其最大的优势在于其在并行化处理上做出的贡献。换"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">梦之图景</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">About</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">梦之图景</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">About</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">News_深度学习BERT</h1>

      <div class="post-meta">
        <span class="post-time"> 2020-08-04 </span>
        <div class="post-category">
            <a href="/categories/nlp/"> NLP </a>
            </div>
          <span class="more-meta"> 6123 words </span>
          <span class="more-meta"> 13 mins read </span>
        
      </div>
    </header>

    
    <div class="post-content">
      <h2 id="transformer">Transformer</h2>
<p>Transformer是一种<strong>完全基于Attention机制来加速深度学习训练过程的算法模型</strong>，其最大的优势在于其在并行化处理上做出的贡献。换句话说，Transformer就是一个<strong>带有self-attention机制的seq2seq 模型</strong>，即输入是一个sequence，输出也是一个sequence的模型。如下图所示：</p>
<p><img src="https://img-blog.csdnimg.cn/20190729105442326.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L09zY2FyNjI4MDg2OA==,size_16,color_FFFFFF,t_70" alt="img"></p>
<p><strong>self-attention的架构</strong></p>
<p>假设有x1、x2、x3、x4x1、x2、x3、x4<em>x</em>1、<em>x</em>2、<em>x</em>3、<em>x</em>4四个序列，首先进行带权乘法ai=Wxiai = W , xi<em>a**i</em>=<em>W<strong>x</strong>i</em>，得到新的序列a1、a2、a3、a4a1、a2、a3、a4<em>a</em>1、<em>a</em>2、<em>a</em>3、<em>a</em>4。</p>
<p><img src="https://img-blog.csdnimg.cn/20190729122430138.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L09zY2FyNjI4MDg2OA==,size_16,color_FFFFFF,t_70" alt="img"></p>
<p>然后将aiai<em>a**i</em>分别乘以三个不同的权重矩阵得到qi、ki、viqi、ki、vi<em>q**i</em>、<em>k**i</em>、<em>v**i</em>三个向量，qi=Wqaiqi=Wq , ai<em>q**i</em>=<em>W<strong>q</strong>a**i</em>，ki=Wqaiki=Wq , ai<em>k**i</em>=<em>W<strong>q</strong>a**i</em>，vi=Wvaivi=Wv , ai<em>v**i</em>=<em>W<strong>v</strong>a**i</em>，qq<em>q</em>表示的是query，需要match其他的向量，kk<em>k</em>表示的是key，是需要被qq<em>q</em>来match的，vv<em>v</em>表示value，表示需要被抽取出来的信息。</p>
<p><img src="https://img-blog.csdnimg.cn/20190729152314974.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L09zY2FyNjI4MDg2OA==,size_16,color_FFFFFF,t_70" alt="img"></p>
<p>接下来让每一个qq<em>q</em>对每一个kk<em>k</em>做attention操作。attention操作的目的是输入两个向量，输出一个数，这里使用scaled点积来实现。q1q1<em>q</em>1和kiki<em>k**i</em>做attention得到α1,i{\alpha}_{1,i}<em>α</em>1,<em>i</em>，</p>
<p>α1,i=q1⋅ki/d‾‾√{\alpha}_{1,i} = q1 \cdot ki / \sqrt{d}<em>α</em>1,<em>i</em>=<em>q</em>1⋅<em>k**i</em>/<em>d</em></p>
<p>其中dd<em>d</em>表示qq<em>q</em>和vv<em>v</em>的维数。</p>
<p><img src="https://img-blog.csdnimg.cn/20190729154406562.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L09zY2FyNjI4MDg2OA==,size_16,color_FFFFFF,t_70" alt="img"></p>
<p>最后将α1,i{\alpha}_{1,i}<em>α</em>1,<em>i</em>带入softmax函数，写成矩阵形式即为：</p>
<p>Attention(Q,K,V)=softmax(QKTd√V)Attention(Q,K,V) = softmax \left( \frac{QK^T}{\sqrt{d}} V \right)<em>A<strong>t</strong>t<strong>e</strong>n<strong>t</strong>i<strong>o</strong>n</em>(<em>Q</em>,<em>K</em>,<em>V</em>)=<em>s<strong>o</strong>f<strong>t</strong>m<strong>a</strong>x</em>(<em>d<strong>Q</strong>K<strong>T</strong>V</em>)</p>
<p><img src="https://img-blog.csdnimg.cn/20190729155856638.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L09zY2FyNjI4MDg2OA==,size_16,color_FFFFFF,t_70" alt="img"></p>
<p>然后求出$b1 = \sum \hat{\alpha}_{1,i} \cdot vi $。整个过程如下图所示：</p>
<p><img src="https://img-blog.csdnimg.cn/20190729160738214.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L09zY2FyNjI4MDg2OA==,size_16,color_FFFFFF,t_70" alt="img"></p>
<p>由上图可知，求b1b1<em>b</em>1的时候需要用到序列中的所有值a1,a2,a3,a4a1,a2,a3,a4<em>a</em>1,<em>a</em>2,<em>a</em>3,<em>a</em>4，但是对序列的每个部分的关注程度有所不同，通过改变αˆ1,i\hat{\alpha}_{1,i}<em>α</em>^1,<em>i</em>前的权重vivi<em>v**i</em>可以对序列的每一部分赋予不同的关注度，对重点关注的部分赋予较大的权重，不太关注的部分赋予较小的权重。
用同样的方法可以求出b2,b3,b4b2,b3,b4<em>b</em>2,<em>b</em>3,<em>b</em>4，而且b1,b2,b3,b4b1,b2,b3,b4<em>b</em>1,<em>b</em>2,<em>b</em>3,<em>b</em>4是平行地被计算出来的，相互之间没有影响。整个过程可以看作是一个self-attention layer，输入x1,x2,x3,x4x1,x2,x3,x4<em>x</em>1,<em>x</em>2,<em>x</em>3,<em>x</em>4，输出b1,b2,b3,b4b1,b2,b3,b4<em>b</em>1,<em>b</em>2,<em>b</em>3,<em>b</em>4.</p>
<p>Transformer所使用的注意力机制的核心思想是去<strong>计算一句话中的每个词对于这句话中所有词的相互关系</strong>，然后认为<strong>这些词与词之间的相互关系在一定程度上反应了这句话中不同词之间的关联性以及重要程度</strong>。因此再利用这些相互关系来调整每个词的重要性（权重）就可以获得每个词新的表达。这个新的表征不但蕴含了该词本身，还蕴含了其他词与这个词的关系，因此和单纯的词向量相比是一个更加全局的表达。使用了Attention机制，将序列中的任意两个位置之间的距离缩小为一个常量。</p>
<p><img src="https://img-blog.csdnimg.cn/20190729162622945.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L09zY2FyNjI4MDg2OA==,size_16,color_FFFFFF,t_70" alt="img"></p>
<p>乘积部分的表示如下图：</p>
<p><img src="https://img-blog.csdnimg.cn/20190729170755467.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L09zY2FyNjI4MDg2OA==,size_16,color_FFFFFF,t_70" alt="img"></p>
<p>整个transformer的结构如下图：</p>
<p><img src="http://5b0988e595225.cdn.sohucs.com/images/20190725/4b086c3175234abbb686a0cebe6012ce.jpeg" alt="img"></p>
<p>左边的部分是encoder，右边的部分是decoder。</p>
<p>encoder部分的步骤如下所示：</p>
<ul>
<li>
<p>对input进行<strong>embedding</strong>操作，将单词表示成长度为embedding size的向量</p>
</li>
<li>
<p>对embedding之后的词向量进行<strong>positional encoding</strong>，即在生成q、k、vq、k、v<em>q</em>、<em>k</em>、<em>v</em>的时候，给每一个aiai<em>a**i</em>加上一个相同维数的向量eiei<em>e**i</em>，如下图：</p>
<p><img src="https://img-blog.csdnimg.cn/20190729223129293.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L09zY2FyNjI4MDg2OA==,size_16,color_FFFFFF,t_70" alt="img"></p>
<p>eiei<em>e**i</em>表示了位置的信息，每一个位置对应不同的eiei<em>e**i</em>，id为p的位置对应的位置向量的公式为：</p>
<p>e2i=sin(p/100002i/d)e_{2i} = sin \left( p/10000^{2i/d} \right)*e*2*i*=*s**i**n*(*p*/100002*i*/*d*)</p>
<p>e2i+1=cos(p/100002i/d)e_{2i+1} = cos \left( p/10000^{2i/d} \right)*e*2*i*+1=*c**o**s*(*p*/100002*i*/*d*)</p>
<p>对于NLP中的任务来说，顺序是很重要的信息，它代表着局部甚至是全局的结构，学习不到顺序信息，那么效果将会大打折扣。通过结合位置向量和词向量，给每个词都引入了一定的位置信息，这样Attention就可以分辨出不同位置的词</p>
</li>
<li>
<p>进行<strong>muti-head attention</strong>操作，同时生成多个q、k、vq、k、v<em>q</em>、<em>k</em>、<em>v</em>分别进行attention（参数不同），然后把结果拼接起来</p>
</li>
<li>
<p><strong>add &amp; norm</strong>操作，将muti-head attention的input和output进行相加，然后进行layer normalization操作</p>
<p>layer normalization(LN) 和batch normalization(BN) 的过程相反，BN表示在一个batch里面的数据相同的维度进行normalization，而LN表示在对每一个数据所有的维度进行normalization操作。假设数据的规模是10行3列，即batchsize = 10，每一行数据有三个特征，假设这三个特征是 [身高、体重、年龄]。那么BN是针对每一列（特征）进行缩放，例如算出[身高]的均值与方差，再对身高这一列的10个数据进行缩放。体重和年龄同理。这是一种“<strong>列缩放</strong>”。而layer方向相反，它针对的是每一行进行缩放。即只看一条记录，算出这条记录所有特征的均值与方差再缩放。这是一种“<strong>行缩放</strong>”</p>
</li>
<li>
<p>然后进行一个<strong>feed forward</strong>和<strong>add &amp; norm</strong>操作，feed forward会对每一个输入的序列进行操作，再进行一个add &amp; norm操作</p>
</li>
</ul>
<p>至此encoder的操作就完成了，接下来看decoder操作：</p>
<ul>
<li>进行<strong>positional encoding</strong></li>
<li><strong>masked muti-head attention</strong>操作，即对之前产生的序列进行attention</li>
<li>进行<strong>add &amp; norm</strong>操作</li>
<li><strong>将encoder的输出和上一轮add &amp; norm操作的结构进行muti-head attention和add &amp; norm操作</strong></li>
<li><strong>feed forward 和 add &amp; norm</strong>，整个的过程可以重复N次</li>
<li>经过<strong>线性层和softmax层</strong>得到最终输出</li>
</ul>
<h2 id="bert">Bert</h2>
<p>Bert(Bidirectional Encoder Representation form Transformers)，即双向Transformer的Encoder，其中“双向”表示模型在处理某一个词时，它能同时利用前面的词和后面的词两部分信息。Bert的模型架构基于多层双向转换解码，通过执行一系列预训练，进而得到深层的上下文表示。Bert的基本思想和Word2Vec、CBOW一样，都是给定context，来预测下一个词。BERT的结构和ELMo相似都是双向结构。BERT模型结构如下图所示：</p>
<p><img src="https://camo.githubusercontent.com/f5297c1c8c1e71180cb62aca463503d21122e911/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303731343231313331363136372e706e67" alt="img"></p>
<p>Bert的实现分为两个阶段：第一个阶段叫做：<strong>Pre-training</strong>，跟WordEmbedding类似，利用现有无标记的语料训练一个语言模型。第二个阶段叫做：<strong>Fine-tuning</strong>，利用预训练好的语言模型，完成具体的NLP下游任务。</p>
<p><strong>Bert的输入：</strong></p>
<p>Bert的输入包含三个部分：<strong>Token Embeddings、Segment Embeddings、Position Embeddings</strong>。这三个部分在整个过程中是可以学习的。</p>
<p><img src="https://camo.githubusercontent.com/1fa48883f724ed604556d3bb9d241511012d1972/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303731343231313334383435362e706e67" alt="img"></p>
<ul>
<li>CLS：Classification Token，用来区分不同的类</li>
<li>SEP：Special Token，用来分隔两个不同的句子</li>
<li>Token Embedding：对输入的单词进行Embedding</li>
<li>Segment Embedding：标记输入的单词是属于句子A还是句子B</li>
<li>Position Embedding：标记每一个Token的位置</li>
</ul>
<p><strong>Pre-training：</strong></p>
<p>Bert的预训练有两个任务：<strong>Masked Language Model（MLM）**和**Next Sentence Predicition（NSP）</strong>。在训练Bert的时候，这两个任务是同时训练的，Bert的损失函数是把这两个任务的损失函数加起来的，是一个多任务训练。</p>
<p>Masked Language Model的灵感来源于完形填空，将15%的Tokens掩盖。被掩盖的15%的Tokens又分为三种情况：80%的字符用字符“MASK”替换，10%的字符用另外的字符替换；10%的字符是保持不动。然后模型尝试基于序列中其他未被掩盖的单词的上下文来预测被掩盖的原单词。最后在计算损失时，只计算被掩盖的15%的Tokens。</p>
<p>Next Sentence Prediction，即给出两个句子A和B，B有一半的可能性是A的下一句话，训练模型来预测B是不是A的下一句话。通过训练，使模型具备理解长序列上下文的联系的能力。</p>
<p><strong>Fine-tuning：</strong></p>
<p><img src="https://camo.githubusercontent.com/265b6f273c9731886f63f66b7beb2805226cee73/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303731343231313430393538322e706e67" alt="img"></p>
<ul>
<li>分类任务：输入端，可以是句子A和句子B也可以是单一句子，CLS后接softmax用于分类</li>
<li>答案抽取，比如SQuAd v1.0，训练一个start和end向量分别为S，E，bert的每个输出向量和S或E计算dot product，之后对所有输出节点的点乘结果进行softmax得到该节点对应位置的起始概率或者或终止概率， 假设Bert的输出向量为TT<em>T</em>，则用S⋅Ti+E⋅TjS·Ti + E·Tj<em>S</em>⋅<em>T**i</em>+<em>E</em>⋅<em>T**j</em>表示从ii<em>i</em>位置起始，jj<em>j</em>位置终止的概率，最大的概率对应ii<em>i</em>和j(i&lt;j)j(i&lt;j)<em>j</em>(<em>i</em>&lt;<em>j</em>)即为预测的answer span的起点终点，训练的目标是最大化正确的起始,终点的概率</li>
<li>SQuAD v2.0和SQuAD 1.1的区别在于可以有答案不存在给定文本中，因此增加CLS的节点输出为CC<em>C</em>，当最大的分数对应i,ji,j<em>i</em>,<em>j</em>所在的CLS的时候，即S⋅Ti+E⋅TjS·Ti + E·Tj<em>S</em>⋅<em>T**i</em>+<em>E</em>⋅<em>T**j</em>的最大值小于S⋅C+E⋅CS·C + E·C<em>S</em>⋅<em>C</em>+<em>E</em>⋅<em>C</em>时，不存在答案</li>
</ul>
<h2 id="代码实现">代码实现</h2>
<p>初始化：</p>
<p>![屏幕快照 2020-08-05 上午12.18.21](/Users/lishanshan/Desktop/屏幕快照 2020-08-05 上午12.18.21.png)</p>
<p>读取数据并划分验证集：</p>
<p>![屏幕快照 2020-08-05 上午12.20.45](/Users/lishanshan/Desktop/屏幕快照 2020-08-05 上午12.20.45.png)</p>
<p>![屏幕快照 2020-08-05 上午12.20.56](/Users/lishanshan/Desktop/屏幕快照 2020-08-05 上午12.20.56.png)</p>
<p>![屏幕快照 2020-08-05 上午12.21.08](/Users/lishanshan/Desktop/屏幕快照 2020-08-05 上午12.21.08.png)</p>
<p>建立训练，验证和测试集：</p>
<p>![屏幕快照 2020-08-05 上午12.22.35](/Users/lishanshan/Desktop/屏幕快照 2020-08-05 上午12.22.35.png)</p>
<p>建立词典：</p>
<p>![屏幕快照 2020-08-05 上午12.23.17](/Users/lishanshan/Desktop/屏幕快照 2020-08-05 上午12.23.17.png)</p>
<p>![屏幕快照 2020-08-05 上午12.23.27](/Users/lishanshan/Desktop/屏幕快照 2020-08-05 上午12.23.27.png)</p>
<p>![屏幕快照 2020-08-05 上午12.23.36](/Users/lishanshan/Desktop/屏幕快照 2020-08-05 上午12.23.36.png)</p>
<p>![屏幕快照 2020-08-05 上午12.23.46](/Users/lishanshan/Desktop/屏幕快照 2020-08-05 上午12.23.46.png)</p>
<p>建立各个模块：</p>
<p>![屏幕快照 2020-08-05 上午12.24.51](/Users/lishanshan/Desktop/屏幕快照 2020-08-05 上午12.24.51.png)</p>
<p>![屏幕快照 2020-08-05 上午12.25.04](/Users/lishanshan/Desktop/屏幕快照 2020-08-05 上午12.25.04.png)</p>
<p>![屏幕快照 2020-08-05 上午12.25.17](/Users/lishanshan/Desktop/屏幕快照 2020-08-05 上午12.25.17.png)</p>
<p>![屏幕快照 2020-08-05 上午12.25.29](/Users/lishanshan/Desktop/屏幕快照 2020-08-05 上午12.25.29.png)</p>
<p>![屏幕快照 2020-08-05 上午12.25.45](/Users/lishanshan/Desktop/屏幕快照 2020-08-05 上午12.25.45.png)</p>
<p>![屏幕快照 2020-08-05 上午12.26.51](/Users/lishanshan/Desktop/屏幕快照 2020-08-05 上午12.26.51.png)</p>
<p>![屏幕快照 2020-08-05 上午12.27.01](/Users/lishanshan/Desktop/屏幕快照 2020-08-05 上午12.27.01.png)</p>
<p>建立优化器：</p>
<p>![屏幕快照 2020-08-05 上午12.27.48](/Users/lishanshan/Desktop/屏幕快照 2020-08-05 上午12.27.48.png)</p>
<p>![屏幕快照 2020-08-05 上午12.27.58](/Users/lishanshan/Desktop/屏幕快照 2020-08-05 上午12.27.58.png)</p>
<p>建立数据库：</p>
<p>![屏幕快照 2020-08-05 上午12.28.39](/Users/lishanshan/Desktop/屏幕快照 2020-08-05 上午12.28.39.png)</p>
<p>![屏幕快照 2020-08-05 上午12.28.51](/Users/lishanshan/Desktop/屏幕快照 2020-08-05 上午12.28.51.png)</p>
<p>划分batch：</p>
<p>![屏幕快照 2020-08-05 上午12.30.03](/Users/lishanshan/Desktop/屏幕快照 2020-08-05 上午12.30.03.png)</p>
<p>![屏幕快照 2020-08-05 上午12.30.13](/Users/lishanshan/Desktop/屏幕快照 2020-08-05 上午12.30.13.png)</p>
<p>其他用到的函数：</p>
<p>![屏幕快照 2020-08-05 上午12.31.04](/Users/lishanshan/Desktop/屏幕快照 2020-08-05 上午12.31.04.png)</p>
<h4 id="训练函数">训练函数</h4>
<p>建立训练器：</p>
<p>![屏幕快照 2020-08-05 上午12.32.11](/Users/lishanshan/Desktop/屏幕快照 2020-08-05 上午12.32.11.png)</p>
<p>![屏幕快照 2020-08-05 上午12.32.20](/Users/lishanshan/Desktop/屏幕快照 2020-08-05 上午12.32.20.png)</p>
<p>![屏幕快照 2020-08-05 上午12.32.30](/Users/lishanshan/Desktop/屏幕快照 2020-08-05 上午12.32.30.png)</p>
<p>![屏幕快照 2020-08-05 上午12.32.40](/Users/lishanshan/Desktop/屏幕快照 2020-08-05 上午12.32.40.png)</p>
<p>![屏幕快照 2020-08-05 上午12.32.48](/Users/lishanshan/Desktop/屏幕快照 2020-08-05 上午12.32.48.png)</p>
<p>![屏幕快照 2020-08-05 上午12.32.59](/Users/lishanshan/Desktop/屏幕快照 2020-08-05 上午12.32.59.png)</p>
<p>![屏幕快照 2020-08-05 上午12.33.08](/Users/lishanshan/Desktop/屏幕快照 2020-08-05 上午12.33.08.png)</p>
<p>![屏幕快照 2020-08-05 上午12.33.19](/Users/lishanshan/Desktop/屏幕快照 2020-08-05 上午12.33.19.png)</p>
<p>进行训练：</p>
<p>![屏幕快照 2020-08-05 上午12.33.57](/Users/lishanshan/Desktop/屏幕快照 2020-08-05 上午12.33.57.png)</p>
<p>训练和测试结果：</p>
<p>![屏幕快照 2020-08-05 上午12.37.42](/Users/lishanshan/Desktop/屏幕快照 2020-08-05 上午12.37.42.png)</p>

    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">Author</span>
    <span class="item-content">rabbitlss</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">LastMod</span>
    <span class="item-content">
        2020-08-04
        
    </span>
  </p>
  
  
</div>
<footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/preview/">preview</a>
          <a href="/tags/nlp/">NLP</a>
          <a href="/tags/tag-5/">tag-5</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/post/%E5%9F%BA%E4%BA%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">News_机器学习下的文本分类常用算法</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        <a class="next" href="/post/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB_fasttext/">
            <span class="next-text nav-default">News_深度学习下的文本分类_FastText</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        <div id="comments-gitment"></div>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/theme-next/theme-next-gitment@1/default.min.css" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/gh/theme-next/theme-next-gitment@1/gitment.browser.min.js" crossorigin="anonymous"></script>
    <script type="text/javascript">
      var gitment = new Gitment({
        id: '2020-08-04 23:10:23 \u002b0800 CST',
        title: 'News_深度学习BERT',
        link: decodeURI(location.href),
        desc: 'Transformer Transformer是一种完全基于Attention机制来加速深度学习训练过程的算法模型，其最大的优势在于其在并行化处理上做出的贡献。换',
        owner: 'rabbitlss',
        repo: '',
        oauth: {
          client_id: '',
          client_secret: ''
        }
      });
      gitment.render('comments-gitment');
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://github.com/imsun/gitment">comments powered by gitment.</a></noscript>

  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="mailto:your@email.com" class="iconfont icon-email" title="email"></a>
      <a href="http://localhost:1313" class="iconfont icon-stack-overflow" title="stack-overflow"></a>
      <a href="http://localhost:1313" class="iconfont icon-twitter" title="twitter"></a>
      <a href="http://localhost:1313" class="iconfont icon-facebook" title="facebook"></a>
      <a href="http://localhost:1313" class="iconfont icon-linkedin" title="linkedin"></a>
      <a href="http://localhost:1313" class="iconfont icon-google" title="google"></a>
      <a href="http://localhost:1313" class="iconfont icon-github" title="github"></a>
      <a href="http://localhost:1313" class="iconfont icon-weibo" title="weibo"></a>
      <a href="http://localhost:1313" class="iconfont icon-zhihu" title="zhihu"></a>
      <a href="http://localhost:1313" class="iconfont icon-douban" title="douban"></a>
      <a href="http://localhost:1313" class="iconfont icon-pocket" title="pocket"></a>
      <a href="http://localhost:1313" class="iconfont icon-tumblr" title="tumblr"></a>
      <a href="http://localhost:1313" class="iconfont icon-instagram" title="instagram"></a>
      <a href="http://localhost:1313" class="iconfont icon-gitlab" title="gitlab"></a>
      <a href="http://localhost:1313" class="iconfont icon-bilibili" title="bilibili"></a>
  <a href="https://rabbitlss.github.io/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2020
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">rabbitlss</span>
  </span>

<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>



<script type="text/javascript" src="/js/main.min.d7b7ada643c9c1a983026e177f141f7363b4640d619caf01d8831a6718cd44ea.js"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      TeX: {equationNumbers: {autoNumber: "AMS"}},
      showProcessingMessages: false,
      messageStyle: 'none'
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"  integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script>








</body>
</html>
